{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b02e0779",
   "metadata": {},
   "source": [
    "# MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5d9dd47d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import confusion_matrix\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6b26e5bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['data', 'target', 'frame', 'feature_names', 'target_names', 'images', 'DESCR'])\n"
     ]
    }
   ],
   "source": [
    "digits = load_digits()\n",
    "print(digits.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "83c61d65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (1797, 64) and (1797,)\n",
      " train set : (1437, 64) and (1437,)\n",
      " test set : (360, 64) and (360,)\n"
     ]
    }
   ],
   "source": [
    "x,y = digits.data, digits.target\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x,y, test_size=0.2)\n",
    "\n",
    "print(f\" {x.shape} and {y.shape}\")\n",
    "print(f\" train set : {x_train.shape} and {y_train.shape}\")\n",
    "print(f\" test set : {x_test.shape} and {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c040b780",
   "metadata": {},
   "source": [
    "# Fit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca3d8ca1",
   "metadata": {},
   "source": [
    "Before scaling :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "abed6f57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 6.71258946\n",
      "Iteration 2, loss = 3.09292459\n",
      "Iteration 3, loss = 1.88809383\n",
      "Iteration 4, loss = 1.16838947\n",
      "Iteration 5, loss = 0.78991118\n",
      "Iteration 6, loss = 0.57656763\n",
      "Iteration 7, loss = 0.45023888\n",
      "Iteration 8, loss = 0.37179109\n",
      "Iteration 9, loss = 0.31766453\n",
      "Iteration 10, loss = 0.27686000\n",
      "Iteration 11, loss = 0.24427437\n",
      "Iteration 12, loss = 0.21731705\n",
      "Iteration 13, loss = 0.19627757\n",
      "Iteration 14, loss = 0.17524792\n",
      "Iteration 15, loss = 0.16034244\n",
      "Iteration 16, loss = 0.14513475\n",
      "Iteration 17, loss = 0.13493067\n",
      "Iteration 18, loss = 0.12179386\n",
      "Iteration 19, loss = 0.11397417\n",
      "Iteration 20, loss = 0.10485219\n",
      "Iteration 21, loss = 0.09730867\n",
      "Iteration 22, loss = 0.09039057\n",
      "Iteration 23, loss = 0.08661291\n",
      "Iteration 24, loss = 0.08212946\n",
      "Iteration 25, loss = 0.07589807\n",
      "Iteration 26, loss = 0.07132494\n",
      "Iteration 27, loss = 0.06517622\n",
      "Iteration 28, loss = 0.06308130\n",
      "Iteration 29, loss = 0.05765402\n",
      "Iteration 30, loss = 0.05688468\n",
      "Iteration 31, loss = 0.05572041\n",
      "Iteration 32, loss = 0.04869696\n",
      "Iteration 33, loss = 0.04564416\n",
      "Iteration 34, loss = 0.04316074\n",
      "Iteration 35, loss = 0.04016559\n",
      "Iteration 36, loss = 0.03795078\n",
      "Iteration 37, loss = 0.03682167\n",
      "Iteration 38, loss = 0.03439624\n",
      "Iteration 39, loss = 0.03223930\n",
      "Iteration 40, loss = 0.03060248\n",
      "Iteration 41, loss = 0.02975599\n",
      "Iteration 42, loss = 0.02908738\n",
      "Iteration 43, loss = 0.02738346\n",
      "Iteration 44, loss = 0.02557837\n",
      "Iteration 45, loss = 0.02421602\n",
      "Iteration 46, loss = 0.02409389\n",
      "Iteration 47, loss = 0.02428112\n",
      "Iteration 48, loss = 0.02180558\n",
      "Iteration 49, loss = 0.02101668\n",
      "Iteration 50, loss = 0.01948610\n",
      "Iteration 51, loss = 0.01888048\n",
      "Iteration 52, loss = 0.01808168\n",
      "Iteration 53, loss = 0.01827544\n",
      "Iteration 54, loss = 0.01685202\n",
      "Iteration 55, loss = 0.01656839\n",
      "Iteration 56, loss = 0.01558111\n",
      "Iteration 57, loss = 0.01547523\n",
      "Iteration 58, loss = 0.01479060\n",
      "Iteration 59, loss = 0.01410188\n",
      "Iteration 60, loss = 0.01355766\n",
      "Iteration 61, loss = 0.01325306\n",
      "Iteration 62, loss = 0.01277914\n",
      "Iteration 63, loss = 0.01250819\n",
      "Iteration 64, loss = 0.01212731\n",
      "Iteration 65, loss = 0.01176804\n",
      "Iteration 66, loss = 0.01114377\n",
      "Iteration 67, loss = 0.01098992\n",
      "Iteration 68, loss = 0.01101831\n",
      "Iteration 69, loss = 0.01034765\n",
      "Iteration 70, loss = 0.00999368\n",
      "Iteration 71, loss = 0.00972740\n",
      "Iteration 72, loss = 0.00938022\n",
      "Iteration 73, loss = 0.00914478\n",
      "Iteration 74, loss = 0.00898916\n",
      "Iteration 75, loss = 0.00873429\n",
      "Iteration 76, loss = 0.00838071\n",
      "Iteration 77, loss = 0.00816787\n",
      "Iteration 78, loss = 0.00804168\n",
      "Iteration 79, loss = 0.00804742\n",
      "Iteration 80, loss = 0.00766404\n",
      "Iteration 81, loss = 0.00737152\n",
      "Iteration 82, loss = 0.00724512\n",
      "Iteration 83, loss = 0.00706850\n",
      "Iteration 84, loss = 0.00709829\n",
      "Iteration 85, loss = 0.00691801\n",
      "Iteration 86, loss = 0.00670635\n",
      "Iteration 87, loss = 0.00642870\n",
      "Iteration 88, loss = 0.00619296\n",
      "Iteration 89, loss = 0.00606679\n",
      "Iteration 90, loss = 0.00588996\n",
      "Iteration 91, loss = 0.00578019\n",
      "Iteration 92, loss = 0.00564859\n",
      "Iteration 93, loss = 0.00555634\n",
      "Iteration 94, loss = 0.00540175\n",
      "Iteration 95, loss = 0.00526098\n",
      "Iteration 96, loss = 0.00519279\n",
      "Iteration 97, loss = 0.00509934\n",
      "Iteration 98, loss = 0.00499990\n",
      "Iteration 99, loss = 0.00491339\n",
      "Iteration 100, loss = 0.00481907\n",
      "Iteration 101, loss = 0.00471605\n",
      "Iteration 102, loss = 0.00470712\n",
      "Iteration 103, loss = 0.00459495\n",
      "Iteration 104, loss = 0.00449351\n",
      "Iteration 105, loss = 0.00442224\n",
      "Iteration 106, loss = 0.00429465\n",
      "Iteration 107, loss = 0.00421119\n",
      "Iteration 108, loss = 0.00412462\n",
      "Iteration 109, loss = 0.00412404\n",
      "Iteration 110, loss = 0.00395553\n",
      "Iteration 111, loss = 0.00395119\n",
      "Iteration 112, loss = 0.00384927\n",
      "Iteration 113, loss = 0.00370307\n",
      "Iteration 114, loss = 0.00366667\n",
      "Iteration 115, loss = 0.00361995\n",
      "Iteration 116, loss = 0.00353371\n",
      "Iteration 117, loss = 0.00350408\n",
      "Iteration 118, loss = 0.00340671\n",
      "Iteration 119, loss = 0.00336127\n",
      "Iteration 120, loss = 0.00336052\n",
      "Iteration 121, loss = 0.00328460\n",
      "Iteration 122, loss = 0.00322580\n",
      "Iteration 123, loss = 0.00315399\n",
      "Iteration 124, loss = 0.00311564\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(verbose=True)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = MLPClassifier(verbose=True)\n",
    "clf.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80e834d2",
   "metadata": {},
   "source": [
    "accuracy :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "20c89795",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy : 1.0\n",
      "test accuracy : 0.9777777777777777\n"
     ]
    }
   ],
   "source": [
    "print(f\"train accuracy : {clf.score(x_train,y_train)}\")\n",
    "print(f\"test accuracy : {clf.score(x_test,y_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "122d26ad",
   "metadata": {},
   "source": [
    "After scaling :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bdf90153",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.     0.     0.125  0.6875 0.75   0.0625 0.     0.     0.     0.125\n",
      " 0.875  0.5625 0.5625 0.5    0.     0.     0.     0.625  0.75   0.\n",
      " 0.8125 0.375  0.     0.     0.     0.4    0.3125 0.125  0.8125 0.125\n",
      " 0.     0.     0.     0.     0.     0.625  0.5625 0.     0.     0.\n",
      " 0.     0.     0.     0.0625 0.625  0.5625 0.0625 0.     0.     0.\n",
      " 0.375  0.4375 0.     0.75   0.375  0.     0.     0.     0.0625 0.75\n",
      " 1.     1.     0.3125 0.    ]\n"
     ]
    }
   ],
   "source": [
    "scaler = MinMaxScaler()\n",
    "scaler.fit(x_train)\n",
    "x_train_scale = scaler.transform(x_train)\n",
    "x_test_scale = scaler.transform(x_test)\n",
    "print(x_train_scale[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "427c978e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 2.26491740\n",
      "Iteration 2, loss = 2.08237388\n",
      "Iteration 3, loss = 1.93320008\n",
      "Iteration 4, loss = 1.79007470\n",
      "Iteration 5, loss = 1.64132577\n",
      "Iteration 6, loss = 1.48979030\n",
      "Iteration 7, loss = 1.34117548\n",
      "Iteration 8, loss = 1.19786355\n",
      "Iteration 9, loss = 1.06696701\n",
      "Iteration 10, loss = 0.95014104\n",
      "Iteration 11, loss = 0.84591908\n",
      "Iteration 12, loss = 0.75652701\n",
      "Iteration 13, loss = 0.68045205\n",
      "Iteration 14, loss = 0.61633139\n",
      "Iteration 15, loss = 0.56058456\n",
      "Iteration 16, loss = 0.51316215\n",
      "Iteration 17, loss = 0.47339882\n",
      "Iteration 18, loss = 0.43843353\n",
      "Iteration 19, loss = 0.40783162\n",
      "Iteration 20, loss = 0.38196238\n",
      "Iteration 21, loss = 0.35931661\n",
      "Iteration 22, loss = 0.33844110\n",
      "Iteration 23, loss = 0.32030951\n",
      "Iteration 24, loss = 0.30551077\n",
      "Iteration 25, loss = 0.28965081\n",
      "Iteration 26, loss = 0.27689526\n",
      "Iteration 27, loss = 0.26491376\n",
      "Iteration 28, loss = 0.25457938\n",
      "Iteration 29, loss = 0.24422927\n",
      "Iteration 30, loss = 0.23554944\n",
      "Iteration 31, loss = 0.22710436\n",
      "Iteration 32, loss = 0.21894101\n",
      "Iteration 33, loss = 0.21201824\n",
      "Iteration 34, loss = 0.20516905\n",
      "Iteration 35, loss = 0.19806617\n",
      "Iteration 36, loss = 0.19229538\n",
      "Iteration 37, loss = 0.18660577\n",
      "Iteration 38, loss = 0.18235206\n",
      "Iteration 39, loss = 0.17667143\n",
      "Iteration 40, loss = 0.17181926\n",
      "Iteration 41, loss = 0.16751381\n",
      "Iteration 42, loss = 0.16324255\n",
      "Iteration 43, loss = 0.15912284\n",
      "Iteration 44, loss = 0.15488088\n",
      "Iteration 45, loss = 0.15142836\n",
      "Iteration 46, loss = 0.14809976\n",
      "Iteration 47, loss = 0.14491635\n",
      "Iteration 48, loss = 0.14172472\n",
      "Iteration 49, loss = 0.13878701\n",
      "Iteration 50, loss = 0.13567945\n",
      "Iteration 51, loss = 0.13256797\n",
      "Iteration 52, loss = 0.12972256\n",
      "Iteration 53, loss = 0.12741914\n",
      "Iteration 54, loss = 0.12530837\n",
      "Iteration 55, loss = 0.12248641\n",
      "Iteration 56, loss = 0.11999188\n",
      "Iteration 57, loss = 0.11798236\n",
      "Iteration 58, loss = 0.11608576\n",
      "Iteration 59, loss = 0.11364514\n",
      "Iteration 60, loss = 0.11156249\n",
      "Iteration 61, loss = 0.10939487\n",
      "Iteration 62, loss = 0.10730405\n",
      "Iteration 63, loss = 0.10541502\n",
      "Iteration 64, loss = 0.10410704\n",
      "Iteration 65, loss = 0.10205679\n",
      "Iteration 66, loss = 0.10041948\n",
      "Iteration 67, loss = 0.09903896\n",
      "Iteration 68, loss = 0.09755347\n",
      "Iteration 69, loss = 0.09579140\n",
      "Iteration 70, loss = 0.09399435\n",
      "Iteration 71, loss = 0.09199334\n",
      "Iteration 72, loss = 0.09055487\n",
      "Iteration 73, loss = 0.08930623\n",
      "Iteration 74, loss = 0.08778018\n",
      "Iteration 75, loss = 0.08679118\n",
      "Iteration 76, loss = 0.08478007\n",
      "Iteration 77, loss = 0.08399445\n",
      "Iteration 78, loss = 0.08305770\n",
      "Iteration 79, loss = 0.08096991\n",
      "Iteration 80, loss = 0.07979674\n",
      "Iteration 81, loss = 0.07861879\n",
      "Iteration 82, loss = 0.07735656\n",
      "Iteration 83, loss = 0.07572857\n",
      "Iteration 84, loss = 0.07525131\n",
      "Iteration 85, loss = 0.07468990\n",
      "Iteration 86, loss = 0.07331232\n",
      "Iteration 87, loss = 0.07166353\n",
      "Iteration 88, loss = 0.07083399\n",
      "Iteration 89, loss = 0.06940774\n",
      "Iteration 90, loss = 0.06851310\n",
      "Iteration 91, loss = 0.06753598\n",
      "Iteration 92, loss = 0.06676496\n",
      "Iteration 93, loss = 0.06562864\n",
      "Iteration 94, loss = 0.06530822\n",
      "Iteration 95, loss = 0.06395996\n",
      "Iteration 96, loss = 0.06305919\n",
      "Iteration 97, loss = 0.06224966\n",
      "Iteration 98, loss = 0.06105468\n",
      "Iteration 99, loss = 0.06010868\n",
      "Iteration 100, loss = 0.05944849\n",
      "Iteration 101, loss = 0.05849309\n",
      "Iteration 102, loss = 0.05849855\n",
      "Iteration 103, loss = 0.05717882\n",
      "Iteration 104, loss = 0.05608493\n",
      "Iteration 105, loss = 0.05552285\n",
      "Iteration 106, loss = 0.05455162\n",
      "Iteration 107, loss = 0.05419908\n",
      "Iteration 108, loss = 0.05301846\n",
      "Iteration 109, loss = 0.05238396\n",
      "Iteration 110, loss = 0.05170313\n",
      "Iteration 111, loss = 0.05126908\n",
      "Iteration 112, loss = 0.05017715\n",
      "Iteration 113, loss = 0.05005530\n",
      "Iteration 114, loss = 0.04890671\n",
      "Iteration 115, loss = 0.04856622\n",
      "Iteration 116, loss = 0.04755531\n",
      "Iteration 117, loss = 0.04687385\n",
      "Iteration 118, loss = 0.04619100\n",
      "Iteration 119, loss = 0.04575211\n",
      "Iteration 120, loss = 0.04508057\n",
      "Iteration 121, loss = 0.04455762\n",
      "Iteration 122, loss = 0.04408217\n",
      "Iteration 123, loss = 0.04335784\n",
      "Iteration 124, loss = 0.04259007\n",
      "Iteration 125, loss = 0.04224437\n",
      "Iteration 126, loss = 0.04172049\n",
      "Iteration 127, loss = 0.04125737\n",
      "Iteration 128, loss = 0.04057214\n",
      "Iteration 129, loss = 0.04000877\n",
      "Iteration 130, loss = 0.03947308\n",
      "Iteration 131, loss = 0.03888676\n",
      "Iteration 132, loss = 0.03849985\n",
      "Iteration 133, loss = 0.03807952\n",
      "Iteration 134, loss = 0.03753206\n",
      "Iteration 135, loss = 0.03697240\n",
      "Iteration 136, loss = 0.03652952\n",
      "Iteration 137, loss = 0.03635118\n",
      "Iteration 138, loss = 0.03587453\n",
      "Iteration 139, loss = 0.03584957\n",
      "Iteration 140, loss = 0.03481641\n",
      "Iteration 141, loss = 0.03437683\n",
      "Iteration 142, loss = 0.03411902\n",
      "Iteration 143, loss = 0.03357041\n",
      "Iteration 144, loss = 0.03312025\n",
      "Iteration 145, loss = 0.03331583\n",
      "Iteration 146, loss = 0.03292593\n",
      "Iteration 147, loss = 0.03227482\n",
      "Iteration 148, loss = 0.03195983\n",
      "Iteration 149, loss = 0.03176011\n",
      "Iteration 150, loss = 0.03116597\n",
      "Iteration 151, loss = 0.03122545\n",
      "Iteration 152, loss = 0.03024128\n",
      "Iteration 153, loss = 0.02985984\n",
      "Iteration 154, loss = 0.02938645\n",
      "Iteration 155, loss = 0.02892397\n",
      "Iteration 156, loss = 0.02848181\n",
      "Iteration 157, loss = 0.02827128\n",
      "Iteration 158, loss = 0.02785299\n",
      "Iteration 159, loss = 0.02768138\n",
      "Iteration 160, loss = 0.02742004\n",
      "Iteration 161, loss = 0.02721487\n",
      "Iteration 162, loss = 0.02700003\n",
      "Iteration 163, loss = 0.02656441\n",
      "Iteration 164, loss = 0.02609319\n",
      "Iteration 165, loss = 0.02589831\n",
      "Iteration 166, loss = 0.02564646\n",
      "Iteration 167, loss = 0.02524861\n",
      "Iteration 168, loss = 0.02484961\n",
      "Iteration 169, loss = 0.02492141\n",
      "Iteration 170, loss = 0.02456090\n",
      "Iteration 171, loss = 0.02414987\n",
      "Iteration 172, loss = 0.02430183\n",
      "Iteration 173, loss = 0.02385960\n",
      "Iteration 174, loss = 0.02321986\n",
      "Iteration 175, loss = 0.02314749\n",
      "Iteration 176, loss = 0.02281966\n",
      "Iteration 177, loss = 0.02268660\n",
      "Iteration 178, loss = 0.02228528\n",
      "Iteration 179, loss = 0.02222359\n",
      "Iteration 180, loss = 0.02192517\n",
      "Iteration 181, loss = 0.02166946\n",
      "Iteration 182, loss = 0.02130487\n",
      "Iteration 183, loss = 0.02102737\n",
      "Iteration 184, loss = 0.02096829\n",
      "Iteration 185, loss = 0.02074810\n",
      "Iteration 186, loss = 0.02044120\n",
      "Iteration 187, loss = 0.02019202\n",
      "Iteration 188, loss = 0.02012539\n",
      "Iteration 189, loss = 0.01995564\n",
      "Iteration 190, loss = 0.01979606\n",
      "Iteration 191, loss = 0.01997691\n",
      "Iteration 192, loss = 0.01897885\n",
      "Iteration 193, loss = 0.01922262\n",
      "Iteration 194, loss = 0.01881937\n",
      "Iteration 195, loss = 0.01876816\n",
      "Iteration 196, loss = 0.01859903\n",
      "Iteration 197, loss = 0.01830646\n",
      "Iteration 198, loss = 0.01813523\n",
      "Iteration 199, loss = 0.01800733\n",
      "Iteration 200, loss = 0.01764090\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Asus\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(verbose=True)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_scale = MLPClassifier(verbose=True)\n",
    "clf_scale.fit(x_train_scale,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7fcd2e5",
   "metadata": {},
   "source": [
    "accuracy :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "60a95a73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy : 0.9986082115518441\n",
      "test accuracy : 0.9805555555555555\n"
     ]
    }
   ],
   "source": [
    "print(f\"train accuracy : {clf_scale.score(x_train,y_train)}\")\n",
    "print(f\"test accuracy : {clf_scale.score(x_test,y_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6f755a80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100,)\n"
     ]
    }
   ],
   "source": [
    "print(clf_scale.hidden_layer_sizes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e380f800",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7180a2f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[39  0  0  0  0  0  0  0  0  0]\n",
      " [ 0 46  0  0  0  0  0  0  1  0]\n",
      " [ 0  0 43  0  0  0  0  0  0  0]\n",
      " [ 0  0  0 35  0  1  0  0  0  0]\n",
      " [ 0  0  0  0 25  0  0  0  0  0]\n",
      " [ 0  0  0  0  0 36  1  0  0  1]\n",
      " [ 0  0  0  0  0  0 28  0  0  0]\n",
      " [ 0  0  0  0  0  0  0 32  0  0]\n",
      " [ 0  1  0  0  0  0  0  0 26  0]\n",
      " [ 0  0  0  0  0  0  0  0  1 44]]\n"
     ]
    }
   ],
   "source": [
    "y_pred = clf_scale.predict(x_test_scale)\n",
    "cm = confusion_matrix(y_test,y_pred)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "edf8d148",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(cm)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e50342cb",
   "metadata": {},
   "source": [
    "# save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1aa90e58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Mlp.pkl']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(clf_scale,filename=\"Mlp.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
